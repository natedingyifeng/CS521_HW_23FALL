{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whsg1XX_OZs6"
      },
      "source": [
        "# Boilerplate\n",
        "\n",
        "Packae installation, loading, and dataloaders. There's also a simple model defined. You can change it your favourite architecture if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1domTvnONqD"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "batch_size = 64\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "## Dataloaders\n",
        "train_dataset = datasets.MNIST('mnist_data/', train=True, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "## Simple NN. You can change this if you want.\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc = nn.Linear(28*28, 200)\n",
        "        self.fc2 = nn.Linear(200,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view((-1, 28*28))\n",
        "        x = F.relu(self.fc(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class Normalize(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return (x - 0.1307)/0.3081\n",
        "\n",
        "# Add the data normalization as a first \"layer\" to the network\n",
        "# this allows us to search for adverserial examples to the real image, rather than\n",
        "# to the normalized image\n",
        "model = nn.Sequential(Normalize(), Net())\n",
        "\n",
        "model = model.to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCmWfZHTO8Oo"
      },
      "source": [
        "# Implement the Attacks\n",
        "\n",
        "Functions are given a simple useful signature that you can start with. Feel free to extend the signature as you see fit.\n",
        "\n",
        "You may find it useful to create a 'batched' version of PGD that you can use to create the adversarial attack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZjvA49yONqP"
      },
      "outputs": [],
      "source": [
        "# The last argument 'targeted' can be used to toggle between a targeted and untargeted attack.\n",
        "def fgsm(model, x, target, eps, targeted=True):\n",
        "    ###############################################\n",
        "    x.requires_grad_()\n",
        "    L = nn.CrossEntropyLoss()\n",
        "    loss = L(model(x), target)\n",
        "    loss.backward()\n",
        "    if targeted:\n",
        "        adv_x = x - eps * torch.sign(x.grad)\n",
        "    else:\n",
        "        adv_x = x + eps * torch.sign(x.grad)\n",
        "    adv_x = torch.clamp(adv_x, x - eps, x + eps)\n",
        "    adv_x = torch.clamp(adv_x, 0, 1)\n",
        "    ###############################################\n",
        "    return adv_x\n",
        "\n",
        "\n",
        "def pgd_untargeted(model, x, labels, k, eps, eps_step):\n",
        "    ###############################################\n",
        "    adv_x = x.clone().detach().requires_grad_()\n",
        "    L = nn.CrossEntropyLoss()\n",
        "    for i in range(k):\n",
        "        adv_x_ = adv_x.clone().detach().requires_grad_()\n",
        "        loss = L(model(adv_x_), labels)\n",
        "        loss.backward()\n",
        "        adv_x = adv_x + eps_step * torch.sign(adv_x_.grad)\n",
        "        adv_x = torch.clamp(adv_x, x - eps, x + eps)\n",
        "        adv_x = torch.clamp(adv_x, 0, 1)\n",
        "    ###############################################\n",
        "    return adv_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mja_AB4RykO"
      },
      "source": [
        "# Implement Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-sw8yKYONqQ"
      },
      "outputs": [],
      "source": [
        "k = 40\n",
        "eps = 0.1\n",
        "eps_step = 0.01\n",
        "num_epochs = 5\n",
        "\n",
        "def train_model(model, num_epochs, enable_defense=True):\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "    opt = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    tot_steps = 0\n",
        "\n",
        "    for epoch in range(1,num_epochs+1):\n",
        "        t1 = time.time()\n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            if enable_defense:\n",
        "                ###############################################\n",
        "                model.eval()\n",
        "                x_batch = pgd_untargeted(model, x_batch, y_batch, k, eps, eps_step)\n",
        "                model.train()\n",
        "                ###############################################\n",
        "\n",
        "            tot_steps += 1\n",
        "            opt.zero_grad()\n",
        "            out = model(x_batch)\n",
        "            batch_loss = ce_loss(out, y_batch)\n",
        "            batch_loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        tot_test, tot_acc = 0.0, 0.0\n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(test_loader):\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            out = model(x_batch)\n",
        "            pred = torch.max(out, dim=1)[1]\n",
        "            acc = pred.eq(y_batch).sum().item()\n",
        "            tot_acc += acc\n",
        "            tot_test += x_batch.size()[0]\n",
        "        t2 = time.time()\n",
        "\n",
        "        print('Epoch %d: Accuracy %.5lf [%.2lf seconds]' % (epoch, tot_acc/tot_test, t2-t1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPMdfEhtR3zm"
      },
      "source": [
        "# Study Accuracy, Quality, etc.\n",
        "\n",
        "Compare the various results and report your observations on the submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufD-ccTFR8R2"
      },
      "outputs": [],
      "source": [
        "print(\"Training original model:\")\n",
        "train_model(model, num_epochs, enable_defense=False)\n",
        "model.eval()\n",
        "# Test FGSM attack on original model\n",
        "tot_test, tot_acc = 0.0, 0.0\n",
        "t1 = time.time()\n",
        "for batch_idx, (x_batch, y_batch) in enumerate(test_loader):\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    x_batch = fgsm(model, x_batch, y_batch, eps, targeted=False)\n",
        "    out = model(x_batch)\n",
        "    pred = torch.max(out, dim=1)[1]\n",
        "    acc = pred.eq(y_batch).sum().item()\n",
        "    tot_acc += acc\n",
        "    tot_test += x_batch.size()[0]\n",
        "t2 = time.time()\n",
        "print('FGSM on original model: Accuracy %.5lf [%.2lf seconds]' % (tot_acc/tot_test, t2-t1))\n",
        "# Test PGD attack on original model\n",
        "tot_test, tot_acc = 0.0, 0.0\n",
        "t1 = time.time()\n",
        "for batch_idx, (x_batch, y_batch) in enumerate(test_loader):\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    x_batch = pgd_untargeted(model, x_batch, y_batch, k, eps, eps_step)\n",
        "    out = model(x_batch)\n",
        "    pred = torch.max(out, dim=1)[1]\n",
        "    acc = pred.eq(y_batch).sum().item()\n",
        "    tot_acc += acc\n",
        "    tot_test += x_batch.size()[0]\n",
        "t2 = time.time()\n",
        "print('PGD on original model: Accuracy %.5lf [%.2lf seconds]' % (tot_acc/tot_test, t2-t1))\n",
        "# PGD adversarial training\n",
        "model.train()\n",
        "print(\"PGD adversarial training:\")\n",
        "train_model(model, num_epochs, enable_defense=True)\n",
        "model.eval()\n",
        "# Test FGSM attack on enhanced model\n",
        "tot_test, tot_acc = 0.0, 0.0\n",
        "t1 = time.time()\n",
        "for batch_idx, (x_batch, y_batch) in enumerate(test_loader):\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    x_batch = fgsm(model, x_batch, y_batch, eps, targeted=False)\n",
        "    out = model(x_batch)\n",
        "    pred = torch.max(out, dim=1)[1]\n",
        "    acc = pred.eq(y_batch).sum().item()\n",
        "    tot_acc += acc\n",
        "    tot_test += x_batch.size()[0]\n",
        "t2 = time.time()\n",
        "print('FGSM on enhanced model: Accuracy %.5lf [%.2lf seconds]' % (tot_acc/tot_test, t2-t1))\n",
        "# Test PGD attack on enhanced model\n",
        "tot_test, tot_acc = 0.0, 0.0\n",
        "t1 = time.time()\n",
        "for batch_idx, (x_batch, y_batch) in enumerate(test_loader):\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "    x_batch = pgd_untargeted(model, x_batch, y_batch, k, eps, eps_step)\n",
        "    out = model(x_batch)\n",
        "    pred = torch.max(out, dim=1)[1]\n",
        "    acc = pred.eq(y_batch).sum().item()\n",
        "    tot_acc += acc\n",
        "    tot_test += x_batch.size()[0]\n",
        "t2 = time.time()\n",
        "print('PGD on enhanced model: Accuracy %.5lf [%.2lf seconds]' % (tot_acc/tot_test, t2-t1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python3 (ml)",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
